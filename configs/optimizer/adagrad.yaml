# @package _group_
class_name: torch.optim.Adagrad
params:
  lr: ${training.lr}
